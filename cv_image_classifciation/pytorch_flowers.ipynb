{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 30\n",
    "LEARNING_RATE = 0.003\n",
    "IMG_SIZE = 64\n",
    "CONV_SIZE = math.floor((((IMG_SIZE-2)/2)-2)/2)\n",
    "\n",
    "TRAIN_DATA_PATH = \"C:/Users/Administrator/Desktop/programming/datasets/images/flowers_train_test/train\"\n",
    "TEST_DATA_PATH = \"C:/Users/Administrator/Desktop/programming/datasets/images/flowers_train_test/test\"\n",
    "\n",
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "test_data_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3813\n",
      "510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ensemble_Net(\n",
       "  (e1): Flower_Net_1(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): AvgPool2d(kernel_size=3, stride=3, padding=1)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (e2): Flower_Net_2(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): AvgPool2d(kernel_size=3, stride=3, padding=1)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (e3): Flower_Net_3(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): AvgPool2d(kernel_size=3, stride=3, padding=1)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (avgpool): AvgPool1d(kernel_size=(1,), stride=(1,), padding=(0,))\n",
       "  (fc1): Linear(in_features=216, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flower_Net_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flower_Net_1,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,8,kernel_size=3,padding=1),nn.AvgPool2d(kernel_size=3,padding=1))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(8,16,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(16,8,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        out = self.flatten(x)\n",
    "        return out\n",
    "\n",
    "class Flower_Net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flower_Net_2,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,16,kernel_size=3,padding=1),nn.AvgPool2d(kernel_size=3,padding=1))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(16,32,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(32,8,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        out = self.flatten(x)\n",
    "        return out\n",
    "    \n",
    "class Flower_Net_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flower_Net_3,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,32,kernel_size=3,padding=1),nn.AvgPool2d(kernel_size=3,padding=1))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(32,8,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(8,8,kernel_size=3,padding=1),nn.MaxPool2d(kernel_size=3,padding=1))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        out = self.flatten(x)\n",
    "        return out\n",
    "\n",
    "class ensemble_Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(ensemble_Net,self).__init__()\n",
    "        f1 = Flower_Net_1()\n",
    "        f2 = Flower_Net_2()\n",
    "        f3 = Flower_Net_3()\n",
    "        self.e1 = f1\n",
    "        self.e2 = f2\n",
    "        self.e3 = f3\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=1)\n",
    "        self.fc1 = nn.Linear(216,30)\n",
    "        self.fc2 = nn.Linear(30,5)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        o1 = self.e1(x)\n",
    "    \n",
    "        o2 = self.e2(x)\n",
    "        o3 = self.e3(x)\n",
    "        x = torch.cat((o1,o2,o3),dim=1)\n",
    "        #print(x.size())\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "model = ensemble_Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "CNNmodel = ConvolutionalNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "TARGET:  tensor([4, 4, 3, 3, 3, 4, 1, 4, 4, 1, 4, 0, 0, 3, 0, 4, 1, 4, 1, 1, 4, 2, 3, 0,\n",
      "        4, 4, 2, 1, 0, 4])\n",
      "OUTPUT:  tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4])\n",
      "Accuracy after  50 steps:  0.22666666666666657\n",
      "TARGET:  tensor([0, 4, 3, 0, 2, 4, 3, 4, 2, 1, 3, 2, 1, 1, 1, 0, 0, 4, 3, 0, 3, 0, 0, 1,\n",
      "        2, 0, 2, 0, 4, 4])\n",
      "OUTPUT:  tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4])\n",
      "Accuracy after  100 steps:  0.22699999999999995\n",
      "Epoch:  0 Loss:  7.258926200866699  Accuracy:  0.23203124999999997\n",
      "Validating.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.2\n",
      "Training....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a6c56006ed42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training....\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "n_epochs = 8\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    acc = 0.0\n",
    "    print(\"Training....\")\n",
    "    model.train()\n",
    "    for batch_num,(batch,labels) in enumerate(train_data_loader):\n",
    "        inp,target = batch.to(DEVICE),labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(inp)\n",
    "        \n",
    "        op = F.softmax(output,dim=1)\n",
    "        \n",
    "        final_op = torch.argmax(op,dim=1)\n",
    "    \n",
    "        acc += torch.sum(final_op==target).item()/len(target)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=(loss.item()/len(batch))\n",
    "        if batch_num%50 ==0 and batch_num!=0:\n",
    "            print(\"TARGET: \",target)\n",
    "            print(\"OUTPUT: \",final_op)\n",
    "            print(\"Accuracy after \",batch_num,\"steps: \",acc/batch_num)\n",
    "        \n",
    "    \n",
    "    acc = acc/len(train_data_loader)\n",
    "    print(\"Epoch: \",epoch,\"Loss: \",train_loss,\" Accuracy: \",acc)\n",
    "    \n",
    "    \n",
    "    eval_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    print(\"Validating.....\")\n",
    "    for batch in test_data_loader:\n",
    "        inp,target = batch[0].to(DEVICE),batch[1].to(DEVICE)\n",
    "        op = F.softmax(model.forward(inp))\n",
    "        final_op = torch.argmax(op,dim=1)\n",
    "        \n",
    "        eval_acc += np.sum(final_op.detach().cpu().numpy()==target.detach().cpu().numpy())/len(target)\n",
    "        \n",
    "    print(\"Validation accuracy: \",eval_acc/len(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvolutionalNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 8, 3, 1)\n",
    "#         self.conv2 = nn.Conv2d(8, 16, 3, 1)\n",
    "#         self.fc1 = nn.Linear(CONV_SIZE**2*16, 256)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.fc3 = nn.Linear(64, 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = x.view(-1, CONV_SIZE**2*16)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# train_loss = []\n",
    "# test_loss = []\n",
    "# train_correct = []\n",
    "# test_correct = []\n",
    "\n",
    "# for i in range(EPOCHS):\n",
    "#     train_count = 0\n",
    "#     test_count = 0\n",
    "    \n",
    "#     for b, (X_train, y_train) in enumerate(train_data_loader):\n",
    "        \n",
    "#         b+=1\n",
    "        \n",
    "#         y_pred = CNNmodel(X_train)\n",
    "#         loss = criterion(y_pred, y_train)\n",
    " \n",
    "#         predicted = torch.max(y_pred.data, 1)[1]\n",
    "#         batch_corr = (predicted == y_train).sum()\n",
    "#         train_count += batch_corr\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if b%30 == 0:\n",
    "#             print(f'epoch: {i}   batch: {int(b/30)}   loss: {loss.item():.6f}')\n",
    "\n",
    "#     train_loss.append(loss)\n",
    "#     train_correct.append(train_count)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for b, (X_test, y_test) in enumerate(test_data_loader):\n",
    "            \n",
    "#             y_val = CNNmodel(X_test)\n",
    "\n",
    "#             predicted = torch.max(y_val.data, 1)[1] \n",
    "#             test_count += (predicted == y_test).sum()\n",
    "\n",
    "#     loss = criterion(y_val, y_test)\n",
    "#     test_loss.append(loss)\n",
    "#     test_correct.append(test_count)\n",
    "\n",
    "# print(f'\\nTime: {time.time() - start_time:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNNmodel.state_dict(), 'cnn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_correct, label='train correct')\n",
    "plt.plot(test_correct, label='test correct')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
